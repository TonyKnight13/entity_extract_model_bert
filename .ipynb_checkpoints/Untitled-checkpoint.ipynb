{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnrecognizedFlagError",
     "evalue": "Unknown command line flag 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-7f871445f85c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mloader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minput_from_line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtrain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mload_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mjunshi_tool\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mauto_mark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\GitHub\\实体抽取模型\\entity_extract_model_bert\\train.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mFLAGS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFLAGS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m5.1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"gradient clip should't be too much\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;32massert\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"dropout rate between 0 and 1\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;32massert\u001b[0m \u001b[0mFLAGS\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlr\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"learning rate must larger than zero\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\platform\\flags.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[1;31m# a flag.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m       \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\absl\\flags\\_flagvalues.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, argv, known_only)\u001b[0m\n\u001b[0;32m    631\u001b[0m       \u001b[0msuggestions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_helpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_flag_suggestions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    632\u001b[0m       raise _exceptions.UnrecognizedFlagError(\n\u001b[1;32m--> 633\u001b[1;33m           name, value, suggestions=suggestions)\n\u001b[0m\u001b[0;32m    634\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmark_as_parsed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnrecognizedFlagError\u001b[0m: Unknown command line flag 'f'"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "#@Time: 2020/7/3 18:22\n",
    "#@Auther:zhaorui\n",
    "#@File: shiti_predict.py\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from utils import create_model, get_logger\n",
    "from model import Model\n",
    "from loader import input_from_line\n",
    "from train import FLAGS, load_config\n",
    "from junshi_tool import auto_mark\n",
    "import json\n",
    "\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
    "\n",
    "def shiti_predict(text_path,output_path):\n",
    "    config = load_config(FLAGS.config_file)\n",
    "    logger = get_logger(FLAGS.log_file)\n",
    "    # limit GPU memory\n",
    "    tf_config = tf.ConfigProto()\n",
    "    tf_config.gpu_options.allow_growth = True\n",
    "    with open(FLAGS.map_file, \"rb\") as f:\n",
    "        tag_to_id, id_to_tag = pickle.load(f)\n",
    "    sess = tf.Session(config=tf_config)\n",
    "    model = create_model(sess, Model, FLAGS.ckpt_path, config, logger)\n",
    "\n",
    "    input_file = open(text_path, 'r', encoding='utf-8')\n",
    "    output_file = open(output_path, 'w', encoding='utf-8')\n",
    "    result_list = []\n",
    "    for i in input_file.readlines():\n",
    "        data_dict = json.loads(i)\n",
    "        index = data_dict[\"id\"]\n",
    "        text = data_dict[\"内容\"]\n",
    "    # sen_list = text.split(\"\\n\")\n",
    "    # sen_list = text.split(\"。\")\n",
    "    # if \"\" in sen_list:\n",
    "    #     sen_list.remove(\"\")\n",
    "    # result_list = []\n",
    "    # for index, sen in enumerate(sen_list):\n",
    "    #     sen_list[index] = sen + '。'\n",
    "    # for index, sen in enumerate(sen_list):\n",
    "        result = model.evaluate_line(sess, input_from_line(text, FLAGS.max_seq_len, tag_to_id),id_to_tag, index)\n",
    "        output_file.write(json.dumps(result, ensure_ascii=False))\n",
    "        output_file.write(\"\\n\")\n",
    "        result_list.append(result)\n",
    "    return result_list\n",
    "\n",
    "\n",
    "def reverse_dabiaoqian(entity_list, sentence_path, result_path):\n",
    "    ship_all_list = []\n",
    "    plane_all_list = []\n",
    "    missile_all_list = []\n",
    "    army_all_list = []\n",
    "    location_all_list = []\n",
    "    port_all_list = []\n",
    "    airport_all_list = []\n",
    "    base_all_list = []\n",
    "    nature_all_list = []\n",
    "\n",
    "    for entity_dict in entity_list:\n",
    "        ship_list = entity_dict[\"舰艇\"]\n",
    "        plane_list = entity_dict[\"飞机\"]\n",
    "        missile_list = entity_dict[\"导弹\"]\n",
    "        army_list = entity_dict[\"战车\"]\n",
    "        location_list = entity_dict[\"地区\"]\n",
    "        port_list = entity_dict[\"港口\"]\n",
    "        airport_list = entity_dict[\"机场\"]\n",
    "        base_list = entity_dict[\"基地\"]\n",
    "        nature_list = entity_dict[\"自然地理\"]\n",
    "\n",
    "        for ship in ship_list:\n",
    "            if ship not in ship_all_list:\n",
    "                ship_all_list.append(ship)\n",
    "        for plane in plane_list:\n",
    "            if plane not in plane_all_list:\n",
    "                plane_all_list.append(plane)\n",
    "        for missile in missile_list:\n",
    "            if missile not in missile_all_list:\n",
    "                missile_all_list.append(missile)\n",
    "        for army in army_list:\n",
    "            if army not in army_all_list:\n",
    "                army_all_list.append(army)\n",
    "        for location in location_list:\n",
    "            if location not in location_all_list:\n",
    "                location_all_list.append(location)\n",
    "        for port in port_list:\n",
    "            if port not in port_all_list:\n",
    "                port_all_list.append(port)\n",
    "        for airport in airport_list:\n",
    "            if airport not in airport_all_list:\n",
    "                airport_all_list.append(airport)\n",
    "        for base in base_list:\n",
    "            if base not in base_all_list:\n",
    "                base_all_list.append(base)\n",
    "        for nature in nature_list:\n",
    "            if nature not in nature_all_list:\n",
    "                nature_all_list.append(nature)\n",
    "\n",
    "    tag_process(sentence_path, result_path)\n",
    "    auto_mark(result_path, ship_all_list, \"1\")\n",
    "    auto_mark(result_path, plane_all_list, \"2\")\n",
    "    auto_mark(result_path, missile_all_list, \"3\")\n",
    "    auto_mark(result_path, army_all_list, \"4\")\n",
    "    auto_mark(result_path, base_all_list, \"5\")\n",
    "    auto_mark(result_path, airport_all_list, \"6\")\n",
    "    auto_mark(result_path, port_all_list, \"7\")\n",
    "    auto_mark(result_path, nature_all_list, \"8\")\n",
    "    auto_mark(result_path, location_all_list, \"9\")\n",
    "\n",
    "\n",
    "def tag_process(origin_path, data_path):\n",
    "    with open(origin_path, \"r\", encoding=\"utf-8\") as origin_file:\n",
    "        text = origin_file.read()\n",
    "    text = text.replace(\" \", \"\").replace(\" \", \"\")\n",
    "    sen_list = text.split(\"\\n\")\n",
    "    # for i in range(0, len(sen_list)):\n",
    "    #     sen_list[i] = sen_list[i] + \"。\"\n",
    "    with open(data_path, \"w\", encoding=\"utf-8\") as data_file:\n",
    "        for sen in sen_list:\n",
    "            for word in sen:\n",
    "                data_file.write(word + \" O\" + \"\\n\")\n",
    "            data_file.write(\"\\n\")\n",
    "\n",
    "\n",
    "def ziO_trans_sentence(input_path,output_path):\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for row in f.readlines():\n",
    "                if row == \"\\n\":\n",
    "                    f_out.write(\"\\n\")\n",
    "                else:\n",
    "                    f_out.write(row[0])\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "    # tf.app.run(main)\n",
    "    #text = \"近日,美国航空界有传言说,美国空军正在评估采购F-15X。这是已有45年历史的F-15鹰式战机的最新升级型号。文章称,五角大楼准备向国会提交采购F-15X战机的预算案,并且计划与波音公司签订采购合同。如果这项采购成功推进,那么F-15X可能成为美国空军自2001年以来采购的首款新式非隐形战斗机。\\n福特号航空母舰造价达约130亿美元，是美国海军有史以来造价最高的一艘舰船。\"\n",
    "    #text = \"福特号航空母舰造价达约130亿美元，是美国海军有史以来造价最高的一艘舰船。\"\n",
    "    # new_text = \"\"\n",
    "    # for i in range(0,len(text)):\n",
    "    #     new_text += text[i] + \" O\" + \"\\n\"\n",
    "    # print(new_text)\n",
    "    # with open(\"junshi_data/test_all_sentence.txt\", 'r', encoding='utf-8') as f:\n",
    "    #     test_data = f.read()\n",
    "    # shiti_predict(test_data)\n",
    "    # entity_list = shiti_predict(test_data)\n",
    "    # reverse_dabiaoqian(entity_list, \"junshi_data/test.txt\", \"junshi_data/result.txt\")\n",
    "    # tag_process(\"test.txt\", \"result.txt\")\n",
    "    # ziO_trans_sentence(\"data\\junshi\\junshishiti_data_test_total.txt\",\"data\\junshi\\junshishiti_test_total_sentence.txt\")\n",
    "\n",
    "\n",
    "    shiti_predict(\"data/junshi3/导弹、太空装备.json\", \"data/junshi3/导弹、太空装备标注结果.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
